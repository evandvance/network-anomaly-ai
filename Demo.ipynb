{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "report_path = os.getcwd() + \"/reports/Monday-WorkingHours.pcap.report.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(report_path, index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_count = df[\"Predictions\"].value_counts().loc[\"BENIGN\"]\n",
    "mal_count = df[\"Predictions\"].value_counts().loc[\"MALICIOUS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictions\n",
       "BENIGN       324834\n",
       "MALICIOUS     30898\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Predictions\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Labels\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, precision_recall_curve, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def score_predictions(y_truth, y_predictions):\n",
    "    score = {}\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_truth, y_predictions)\n",
    "    score[\"prauc\"] = auc(recall, precision)\n",
    "    score[\"precision\"] = precision_score(y_truth, y_predictions)\n",
    "    score[\"recall\"] = recall_score(y_truth, y_predictions)\n",
    "    score[\"accuracy\"] = accuracy_score(y_truth, y_predictions)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Predictions\"] = le.fit_transform(df[\"Predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evanvance/miniconda3/envs/ids-ai/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/home/evanvance/miniconda3/envs/ids-ai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prauc': 0.5, 'precision': 0.0, 'recall': 0.0, 'accuracy': 0.91314247804527}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_predictions(df[\"Labels\"], df[\"Predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Precision and Recall are 0.0 because there are no attacks inside the analyzed capture. We can force a computation though with meta-knowledge\n",
    "\n",
    "I spent quite a few hours trying to put together a way to label the attacks in the netflow generated from the packet capture file, but I was unable to. I think I am missing a key peice in labeling it. I dont have access to the router logs, just captures from an interface that appears to be between the router/firewall and the rest of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the benign count is the ammount the model predicted correctly and the mal count is the amount the model predicted incorrectly, we can associate benign count with true positive and mal count with false negatives.\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91314247804527"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = benign_count / ( benign_count + mal_count)\n",
    "\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the same as our accuracy since accuracy is\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{ TotalGuesses }\n",
    "$$\n",
    "\n",
    "and TN is undefined and $$Total Guesses = benign count + mal count$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "and we have no TP, precision is 0 since\n",
    "\n",
    "$$\n",
    "\n",
    "0 = \\frac{0}{0+malcount}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
