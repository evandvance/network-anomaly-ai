{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/nima/Downloads/archive-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)  \n",
    "    dataframes.append(df)    # Append the DataFrame to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
      "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
      "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
      "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
      "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
      "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
      "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
      "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
      "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
      "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
      "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
      "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
      "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
      "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
      "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
      "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
      "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
      "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
      "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
      "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
      "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
      "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
      "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
      "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
      "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
      "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
      "       ' Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dataframes[0].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1:\n",
      "  Total Labels: 288602\n",
      "  BENIGN Labels: 288566\n",
      "  Anomalous Labels: 36\n",
      "------------------------------\n",
      "File 2:\n",
      "  Total Labels: 529918\n",
      "  BENIGN Labels: 529918\n",
      "  Anomalous Labels: 0\n",
      "------------------------------\n",
      "File 3:\n",
      "  Total Labels: 191033\n",
      "  BENIGN Labels: 189067\n",
      "  Anomalous Labels: 1966\n",
      "------------------------------\n",
      "File 4:\n",
      "  Total Labels: 286467\n",
      "  BENIGN Labels: 127537\n",
      "  Anomalous Labels: 158930\n",
      "------------------------------\n",
      "File 5:\n",
      "  Total Labels: 225745\n",
      "  BENIGN Labels: 97718\n",
      "  Anomalous Labels: 128027\n",
      "------------------------------\n",
      "File 6:\n",
      "  Total Labels: 445909\n",
      "  BENIGN Labels: 432074\n",
      "  Anomalous Labels: 13835\n",
      "------------------------------\n",
      "File 7:\n",
      "  Total Labels: 692703\n",
      "  BENIGN Labels: 440031\n",
      "  Anomalous Labels: 252672\n",
      "------------------------------\n",
      "File 8:\n",
      "  Total Labels: 170366\n",
      "  BENIGN Labels: 168186\n",
      "  Anomalous Labels: 2180\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"File {i+1}:\")\n",
    "    # Count the total occurrences of each unique label value\n",
    "    label_counts = df[' Label'].value_counts()\n",
    "    # Get the total number of rows (total labels)\n",
    "    total_labels = df[' Label'].count()\n",
    "\n",
    "    benign_count = label_counts.get('BENIGN', 0)  \n",
    "    print(f\"  Total Labels: {total_labels}\")\n",
    "    print(f\"  BENIGN Labels: {benign_count}\")\n",
    "    print(f\"  Anomalous Labels: {total_labels - benign_count}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "combined_df.columns = combined_df.columns.str.strip()\n",
    "\n",
    "\n",
    "X_combined = combined_df.drop(columns=['Label'])  # Drop the 'Label' column\n",
    "y_combined = combined_df['Label']  \n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_combined.replace([float('inf'), float('-inf')], float('nan'), inplace=True)\n",
    "\n",
    "X_combined.dropna(inplace=True)\n",
    "#drop the corresponding y values\n",
    "y_encoded = y_encoded[X_combined.index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the combined data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9986049620210192\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    453804\n",
      "           1       0.85      0.75      0.80       343\n",
      "           2       1.00      1.00      1.00     25795\n",
      "           3       1.00      0.99      0.99      2040\n",
      "           4       1.00      1.00      1.00     46361\n",
      "           5       0.99      0.99      0.99      1149\n",
      "           6       1.00      1.00      1.00      1183\n",
      "           7       1.00      1.00      1.00      1625\n",
      "           9       1.00      0.67      0.80         6\n",
      "          10       0.99      1.00      0.99     31614\n",
      "          11       1.00      1.00      1.00      1187\n",
      "          12       0.76      0.85      0.80       317\n",
      "          13       0.50      0.20      0.29         5\n",
      "          14       0.56      0.39      0.46       147\n",
      "\n",
      "    accuracy                           1.00    565576\n",
      "   macro avg       0.90      0.85      0.87    565576\n",
      "weighted avg       1.00      1.00      1.00    565576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIS377",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
